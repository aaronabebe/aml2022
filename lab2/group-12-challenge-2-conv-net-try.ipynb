{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport glob\nfrom tqdm import tqdm\nimport librosa\nimport numpy as np\nimport sys","metadata":{"execution":{"iopub.status.busy":"2022-05-24T11:53:23.977436Z","iopub.execute_input":"2022-05-24T11:53:23.977793Z","iopub.status.idle":"2022-05-24T11:53:23.983513Z","shell.execute_reply.started":"2022-05-24T11:53:23.977758Z","shell.execute_reply":"2022-05-24T11:53:23.982605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# arguments\n\nN_MELS = 128\nFRAMES = 313\nN_FFT = 1024\nHOP_LENGTH=512\nWIN_LENGTH = 1024\nPOWER = 2.0\nSR = 16000\n\nNUM_CLASS = 41\n\nBATCH_SIZE = 128\nEPOCHS = 1\nLR = 1e-4","metadata":{"execution":{"iopub.status.busy":"2022-05-24T12:39:52.26097Z","iopub.execute_input":"2022-05-24T12:39:52.261631Z","iopub.status.idle":"2022-05-24T12:39:52.267866Z","shell.execute_reply.started":"2022-05-24T12:39:52.261564Z","shell.execute_reply":"2022-05-24T12:39:52.266806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def file_to_vector_array(file_name,\n                         n_mels=N_MELS,\n                         frames=FRAMES,\n                         n_fft=N_FFT,\n                         hop_length=HOP_LENGTH,\n                         power=POWER,\n                         sr=SR):\n    # 01 calculate the number of dimensions\n    dims = n_mels * frames\n\n    # 02 generate melspectrogram using librosa\n    y, sr = librosa.load(file_name, sr=sr, mono=False)\n    mel_spectrogram = librosa.feature.melspectrogram(y=y,\n                                                     sr=sr,\n                                                     n_fft=n_fft,\n                                                     hop_length=hop_length,\n                                                     n_mels=n_mels,\n                                                     power=power)\n\n    # 03 convert melspectrogram to log mel energy\n    log_mel_spectrogram = 20.0 / power * np.log10(mel_spectrogram + sys.float_info.epsilon)\n\n    # 04 calculate total vector size\n    vector_array_size = len(log_mel_spectrogram[0, :]) - frames + 1\n\n    # 05 skip too short clips\n    if vector_array_size < 1:\n        return np.empty((0, dims))\n\n    # 06 generate feature vectors by concatenating multiframes\n    vector_array = np.zeros((vector_array_size, dims))\n    for t in range(frames):\n        vector_array[:, n_mels * t: n_mels * (t + 1)] = log_mel_spectrogram[:, t: t + vector_array_size].T\n\n    return vector_array","metadata":{"execution":{"iopub.status.busy":"2022-05-24T12:10:58.228176Z","iopub.execute_input":"2022-05-24T12:10:58.228615Z","iopub.status.idle":"2022-05-24T12:10:58.24071Z","shell.execute_reply.started":"2022-05-24T12:10:58.228571Z","shell.execute_reply":"2022-05-24T12:10:58.239339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_to_vector_array(file_list,\n                         msg=\"calc...\",\n                         n_mels=N_MELS,\n                         frames=FRAMES,\n                         n_fft=N_FFT,\n                         hop_length=HOP_LENGTH,\n                         power=POWER):\n    # calculate the number of dimensions\n    dims = n_mels * frames\n\n    # iterate file_to_vector_array()\n    for idx in tqdm(range(len(file_list)), desc=msg):\n        vector_array = file_to_vector_array(file_list[idx],\n                                            n_mels=n_mels,\n                                            frames=frames,\n                                            n_fft=n_fft,\n                                            hop_length=hop_length,\n                                            power=power)\n        if idx == 0:\n            dataset = np.zeros((vector_array.shape[0] * len(file_list), dims), np.float32)\n        dataset[vector_array.shape[0] * idx: vector_array.shape[0] * (idx + 1), :] = vector_array\n\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2022-05-24T12:14:04.678453Z","iopub.execute_input":"2022-05-24T12:14:04.678826Z","iopub.status.idle":"2022-05-24T12:14:04.688246Z","shell.execute_reply.started":"2022-05-24T12:14:04.678788Z","shell.execute_reply":"2022-05-24T12:14:04.686933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def file_list_generator(target_dir,\n                        dir_name=\"train\",\n                        ext=\"wav\"):\n    print(\"target_dir : {}\".format('/'.join(str(target_dir).split('/')[-2:])))\n\n    # generate training list\n    training_list_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n    files = sorted(glob.glob(training_list_path))\n    if len(files) == 0:\n        print(f\"{training_list_path} -> no_wav_file!!\")\n\n    print(\"# of training samples : {num}\".format(num=len(files)))\n    return files","metadata":{"execution":{"iopub.status.busy":"2022-05-24T12:14:07.852448Z","iopub.execute_input":"2022-05-24T12:14:07.852825Z","iopub.status.idle":"2022-05-24T12:14:07.863564Z","shell.execute_reply.started":"2022-05-24T12:14:07.852776Z","shell.execute_reply":"2022-05-24T12:14:07.862569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchaudio\nimport torch \n\ndef transform_sample(x, sr=SR):\n    x = x[:sr * 10]  # (1, audio_length)\n    x_wav = torch.from_numpy(x)\n    x_mel = mel_transform(x_wav)\n    x_mel = amplitude_to_db(x_mel)\n    x_mel = x_mel.unsqueeze(0)\n    return x_wav, x_mel, 0\n\n\nclass SliderDataset(torch.utils.data.Dataset):\n    def __init__(self, files):\n        self.X = list_to_vector_array(files)\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=SR,\n                                                      win_length=WIN_LENGTH,\n                                                      hop_length=HOP_LENGTH,\n                                                      n_fft=N_FFT,\n                                                      n_mels=N_MELS,\n                                                      power=POWER)\n        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB(stype='power')\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, index, sr=SR):\n        x = self.X[index]\n        x = x[:sr * 10]  # (1, audio_length)\n        x_wav = torch.from_numpy(x)\n        #x_mel = self.mel_transform(x_wav)\n        #x_mel = self.amplitude_to_db(x_mel)\n        x_mel = x_wav.unsqueeze(0)\n        return x_wav, x_mel\n\ntarget_dir = '../input/eurecom-aml-2022-challenge-2/dev_data/dev_data/slider'\n\nfiles = file_list_generator(target_dir)\nds = SliderDataset(files)\nloader = torch.utils.data.DataLoader(ds, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     pin_memory=True,\n                                     drop_last=True,\n                                     num_workers=4)\n\n","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-05-24T13:49:13.012952Z","iopub.execute_input":"2022-05-24T13:49:13.013897Z","iopub.status.idle":"2022-05-24T13:50:04.075581Z","shell.execute_reply.started":"2022-05-24T13:49:13.013845Z","shell.execute_reply":"2022-05-24T13:50:04.074826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the Convolutional Autoencoder\nclass ConvAutoencoder(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder, self).__init__()\n       \n        self.encoder = nn.Sequential(\n            nn.Conv2d(128, 256, 3, padding=1, stride=(2,2)),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.Conv2d(256, 512, 3, padding=1, stride=(2,2)),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.Conv2d(512, 512, 3, padding=1, stride=(2,2)),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True)\n        )\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 128, 3, padding=1, stride=(2,2)),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.Conv2d(128, 64, 3, padding=1, stride=(2,2)),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.Conv2d(64, 32, 3, padding=1, stride=(2,2)),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.Conv2d(32, 2, 3, padding=1, stride=(1,2)),\n            nn.BatchNorm2d(32),\n            nn.ReLU(True),\n            nn.Conv2d(1, 1, 3, padding=1, stride=(1,2))\n        )\n\n\n    def forward(self, x):\n        print(x.shape)\n        x = self.encoder(x)\n        x = self.decoder(x)      \n        return x","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2022-05-24T14:26:24.827189Z","iopub.execute_input":"2022-05-24T14:26:24.827621Z","iopub.status.idle":"2022-05-24T14:26:24.845915Z","shell.execute_reply.started":"2022-05-24T14:26:24.827585Z","shell.execute_reply":"2022-05-24T14:26:24.844799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = ConvAutoencoder()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\ncriterion = torch.nn.CrossEntropyLoss().to(device)\n\n\nfor epoch in tqdm(range(EPOCHS)):\n    total_loss = 0\n    pbar = tqdm(loader, total=len(loader))\n    for waveform, melspec in pbar:\n        \n        waveform = waveform.float().unsqueeze(1).to(device)\n        melspec = melspec.float().to(device)\n        \n        output = model(melspec)\n        loss = criterion(output, melspec)\n        pbar.set_description(f'Epoch:{epoch_counter}'\n                             f'\\tLclf:{loss.item():.5f}\\t')\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.data\n        \n        print(f\"Epoch: {epoch_counter}\\tLoss: {loss.item()}\")\n        if epoch % 10 == 0:\n            # save model checkpoints\n            checkpoint_name = 'checkpoint_best.pth.tar'\n            state = {\n                'epoch': epoch,\n                'clf_state_dict': model.module.state_dict(),\n                'optimizer': optimizer.state_dict(),\n            }\n            filename='checkpoint.pth.tar'\n            torch.save(state, filename)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T14:26:27.521903Z","iopub.execute_input":"2022-05-24T14:26:27.522423Z","iopub.status.idle":"2022-05-24T14:26:28.33042Z","shell.execute_reply.started":"2022-05-24T14:26:27.522386Z","shell.execute_reply":"2022-05-24T14:26:28.328621Z"},"trusted":true},"execution_count":null,"outputs":[]}]}